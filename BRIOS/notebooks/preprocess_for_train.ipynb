{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import generic_filter\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import rasterio\n",
    "from datetime import datetime, timedelta\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fillNA(rvi_data_path, vh_data_path, rvi_full_path, vh_full_path):\n",
    "    def nanmean_filter(values):\n",
    "        valid_values = values[~np.isnan(values)]\n",
    "        return np.mean(valid_values) if valid_values.size > 0 else np.nan\n",
    "\n",
    "    rvi_data = np.load(rvi_data_path)\n",
    "    vh_data = np.load(vh_data_path)\n",
    "\n",
    "    cnt = 0\n",
    "    for x in range(vh_data.shape[0]):\n",
    "        for y in range(vh_data.shape[1]):\n",
    "            for t in range(vh_data.shape[2]):\n",
    "                if np.isnan(vh_data[x, y, t]):\n",
    "                    cnt += 1\n",
    "                \n",
    "    print(f'truoc khi fill co {cnt} gia tri nan')\n",
    "\n",
    "    data = rvi_data[...,1]\n",
    "\n",
    "    filled_data = data.copy()\n",
    "    nan_positions = np.isnan(np.array(filled_data))\n",
    "    filled_data[nan_positions] = generic_filter(filled_data, nanmean_filter, size=7)[nan_positions]\n",
    "\n",
    "    rvi_filled_data = rvi_data.copy()\n",
    "    for i in tqdm(range(rvi_data.shape[2]), desc='filling rvi dataset:   '):\n",
    "        nan_pos = np.isnan(rvi_filled_data[...,i])\n",
    "        rvi_filled_data[nan_pos,i] = generic_filter(rvi_filled_data[...,i], nanmean_filter, size=7)[nan_pos]\n",
    "\n",
    "    vh_filled_data = vh_data.copy()\n",
    "    for i in tqdm(range(vh_data.shape[2]), desc='filling vh dataset:   '):\n",
    "        nan_pos_vh = np.isnan(vh_filled_data[...,i])\n",
    "        vh_filled_data[nan_pos_vh,i] = generic_filter(vh_filled_data[...,i], nanmean_filter, size=7)[nan_pos_vh]\n",
    "\n",
    "    cnt = 0\n",
    "    for x in range(vh_filled_data.shape[0]):\n",
    "        for y in range(vh_filled_data.shape[1]):\n",
    "            for t in range(vh_filled_data.shape[2]):\n",
    "                if np.isnan(vh_filled_data[x, y, t]):\n",
    "                    cnt += 1\n",
    "                \n",
    "    print(f'sau khi fill con {cnt} gia tri nan')\n",
    "\n",
    "    np.save(rvi_full_path, rvi_filled_data)\n",
    "    np.save(vh_full_path, vh_filled_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_data(dir):\n",
    "    def find_missing_date(list_ndvi, list_rvi):\n",
    "        dates_ndvi = [f for f in list_ndvi if f.endswith('.tif')]\n",
    "        dates_ndvi = sorted(\n",
    "            datetime.strptime(f.split('_')[1].split('.')[0], \"%Y-%m-%d\") for f in dates_ndvi\n",
    "        )\n",
    "    \n",
    "        dates_rvi = [f.split('.')[0].split('_')[-1] for f in list_rvi]\n",
    "    \n",
    "        dates_rvi = sorted(\n",
    "            datetime.strptime(f, \"%Y-%m-%d\") for f in dates_rvi\n",
    "        )\n",
    "        \n",
    "        # Chuyển danh sách về dạng set để so sánh\n",
    "        set_rvi = set(dates_rvi)\n",
    "        set_ndvi = set(dates_ndvi)\n",
    "\n",
    "        # Tìm các ngày bị thiếu trong NDVI nhưng có trong RVI\n",
    "        missing_dates = set_rvi - set_ndvi\n",
    "        print(missing_dates)\n",
    "        return list(missing_dates), dates_rvi\n",
    "            \n",
    "    def create_ndvi_time_series(folder_path, output_path, missing_dates, date_rvi):\n",
    "        \"\"\"\n",
    "        Create a time-series NDVI dataset from a folder of raster files.\n",
    "\n",
    "        This function reads NDVI raster files from a specified folder, handles missing dates by \n",
    "        adding arrays filled with NaN values, and saves the final time-series dataset as a \n",
    "        NumPy array. The time-series data is saved with the shape (height, width, time).\n",
    "\n",
    "        Parameters:\n",
    "            folder_path (str): Path to the folder containing NDVI `.tif` files.\n",
    "                - Files should be named in the format `ndvi8days_YYYY-MM-DD.tif`.\n",
    "            output_path (str): Path to save the resulting `.npy` file.\n",
    "\n",
    "        Processing Details:\n",
    "            1. Identifies missing dates from a predefined list and assigns a NaN-filled array for those dates.\n",
    "            2. Reads NDVI data from available `.tif` files, using the first band of each file.\n",
    "            3. Assumes an 8-day interval between data points and generates a complete time-series.\n",
    "            4. Combines all data into a single NumPy array with the shape `(height, width, time)`.\n",
    "\n",
    "        Outputs:\n",
    "            - A `.npy` file containing the time-series NDVI data.\n",
    "            - Prints a message confirming the save location.\n",
    "\n",
    "        Notes:\n",
    "            - If a date is missing or a corresponding `.tif` file is not found, the function fills that\n",
    "            time step with an array of NaN values.\n",
    "            - The dimensions of the output data are based on the first `.tif` file in the folder.\n",
    "\n",
    "        Example:\n",
    "            create_ndvi_time_series(\n",
    "                folder_path=\"/path/to/ndvi_rasters\",\n",
    "                output_path=\"/path/to/output/ndvi_timeseries.npy\"\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        # Dates with no data\n",
    "        missing_dates = set(missing_dates)  # Convert to set for quick lookups\n",
    "\n",
    "        # Check raster dimensions from a sample file\n",
    "        with rasterio.open(os.path.join(folder_path, os.listdir(folder_path)[0])) as src:\n",
    "            height, width = src.shape\n",
    "\n",
    "        # Initialize list to hold time series data\n",
    "        time_series = []\n",
    "\n",
    "        # Generate the complete list of dates, assuming an 8-day interval\n",
    "        start_date = date_rvi[0]\n",
    "        end_date = date_rvi[-1]\n",
    "        current_date = start_date\n",
    "\n",
    "        while current_date <= end_date:\n",
    "            date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "            file_path = os.path.join(folder_path, f\"ndvi8days_{date_str}.tif\")\n",
    "            \n",
    "            if date_str in missing_dates or not os.path.exists(file_path):\n",
    "                # Append a null array for missing dates\n",
    "                null_array = np.full((height, width), np.nan)\n",
    "                time_series.append(null_array)\n",
    "            else:\n",
    "                # Read and store data for available dates\n",
    "                with rasterio.open(file_path) as src:\n",
    "                    time_series.append(src.read(1))  # Reads the first band\n",
    "\n",
    "            # Increment by 8 days\n",
    "            current_date += timedelta(days=8)\n",
    "\n",
    "        # Stack along the time dimension, then transpose to (x, y, time)\n",
    "        ndvi_data = np.stack(time_series, axis=0).transpose(1, 2, 0)\n",
    "\n",
    "        np.save(output_path, ndvi_data)\n",
    "\n",
    "        print(f\"Data saved to {output_path}\")\n",
    "\n",
    "    \n",
    "    def create_sar_time_series(folder_path, output_rvi_path, output_vh_path):\n",
    "        \"\"\"\n",
    "        Create time-series datasets for RVI and VH from SAR raster files.\n",
    "\n",
    "        This function reads dual-band SAR raster files from a specified folder, extracts \n",
    "        RVI data from the first band and VH data from the second band, and saves each as \n",
    "        a separate time-series `.npy` file. The resulting arrays are shaped `(height, width, time)`.\n",
    "\n",
    "        Parameters:\n",
    "            folder_path (str): Path to the folder containing SAR `.tif` files.\n",
    "                - Files should be named consistently to ensure correct temporal ordering.\n",
    "            output_rvi_path (str): Path to save the resulting RVI time-series `.npy` file.\n",
    "            output_vh_path (str): Path to save the resulting VH time-series `.npy` file.\n",
    "\n",
    "        Processing Details:\n",
    "            1. Lists all `.tif` files in the folder and sorts them for temporal consistency.\n",
    "            2. Extracts RVI data (band 1) and VH data (band 2) from each raster file.\n",
    "            3. Stacks the extracted data along the time dimension.\n",
    "            4. Saves the resulting RVI and VH time-series as separate `.npy` files.\n",
    "\n",
    "        Outputs:\n",
    "            - Two `.npy` files containing the RVI and VH time-series data, respectively.\n",
    "            - Prints the shapes of the resulting arrays and the paths where they are saved.\n",
    "\n",
    "        Notes:\n",
    "            - Assumes that all `.tif` files in the folder have the same spatial dimensions and band structure.\n",
    "            - Temporal ordering of files is based on the alphabetical order of filenames.\n",
    "\n",
    "        Example:\n",
    "            create_sar_time_series(\n",
    "                folder_path=\"/path/to/sar_rasters\",\n",
    "                output_rvi_path=\"/path/to/output/rvi_timeseries.npy\",\n",
    "                output_vh_path=\"/path/to/output/vh_timeseries.npy\"\n",
    "            )\n",
    "        \"\"\"\n",
    "        # List all available .tif files in the folder\n",
    "        available_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.tif')])\n",
    "\n",
    "        # Check raster dimensions from a sample file\n",
    "        with rasterio.open(os.path.join(folder_path, available_files[0])) as src:\n",
    "            height, width = src.shape\n",
    "\n",
    "        # Initialize lists to hold RVI and VH time series data\n",
    "        rvi_time_series = []\n",
    "        vh_time_series = []\n",
    "\n",
    "        # Read and store RVI and VH data from each file\n",
    "        for file_name in available_files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with rasterio.open(file_path) as src:\n",
    "                # Append the first band for RVI data\n",
    "                rvi_time_series.append(src.read(1))  # RVI is in band 1\n",
    "                # Append the second band for VH data\n",
    "                vh_time_series.append(src.read(2))  # VH is in band 2\n",
    "\n",
    "        # Stack along the time dimension and transpose to (x, y, time)\n",
    "        rvi_data = np.stack(rvi_time_series, axis=0).transpose(1, 2, 0)\n",
    "        vh_data = np.stack(vh_time_series, axis=0).transpose(1, 2, 0)\n",
    "\n",
    "        # Save each time series as .npy file\n",
    "        np.save(output_rvi_path, rvi_data)\n",
    "        np.save(output_vh_path, vh_data)\n",
    "\n",
    "        print(f\"RVI data saved to {output_rvi_path}\")\n",
    "        print(f\"VH data saved to {output_vh_path}\")\n",
    "    \n",
    "    def choose_data_validate(arr_index, valid_ratio = 0.1):\n",
    "        num_valid = max(1, int(valid_ratio * len(arr_index)))\n",
    "        index_for_valid = np.random.choice(arr_index, num_valid, replace=False)\n",
    "        return index_for_valid\n",
    "\n",
    "    def cal_timestep(time, mask):\n",
    "        \"\"\"calculate timestep (between step t to step t-i nearest without cloud)\n",
    "\n",
    "        Args:\n",
    "            time (_type_): _description_\n",
    "            mask (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            deltaT: timestep\n",
    "        \"\"\"\n",
    "        deltaT = time.copy()\n",
    "        for i in range(len(time)):\n",
    "            T_time0 = time[i]\n",
    "            if i != 0:\n",
    "                for k in range(i - 1, -1, -1):\n",
    "                    T_time1 = time[k]\n",
    "                    if mask[k] == 1:\n",
    "                        T_time1 = time[k]\n",
    "                        break\n",
    "\n",
    "                T = T_time0-T_time1\n",
    "            else:\n",
    "                T = 0\n",
    "\n",
    "            deltaT[i] = T\n",
    "\n",
    "        return deltaT\n",
    "\n",
    "    def parse_rec(values, masks, eval_masks, deltas):\n",
    "        rec = []\n",
    "        for i in range(values.shape[0]):\n",
    "            recone = {}\n",
    "            recone['deltas'] = deltas[i, :].tolist()\n",
    "            recone['masks'] = masks[i].astype('int8').tolist()\n",
    "            recone['values'] = values[i, :].astype('float32').tolist()\n",
    "            recone['eval_masks'] = eval_masks[i].astype('int8').tolist()\n",
    "            rec.append(recone)\n",
    "        return rec\n",
    "    \n",
    "    def parse_idTrain(id_):\n",
    "        values = traindatasets_valuesF[:, :, id_]\n",
    "        masks = traindatasets_maskF[:, id_]\n",
    "        eval_masks = traindatasets_evalmaskF[:, id_]\n",
    "        deltas = traindatasets_deltaF[:, :, id_]\n",
    "        deltasB = traindatasets_deltaBF[:, :, id_]\n",
    "\n",
    "        rec = {}\n",
    "\n",
    "        rec['forward'] = parse_rec(values, masks, eval_masks, deltas)\n",
    "        rec['backward'] = parse_rec(values[::-1], masks[::-1], eval_masks[::-1], deltasB)\n",
    "\n",
    "        rec = json.dumps(rec)\n",
    "        fs.write(rec + '\\n')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Step create time series data\n",
    "    ndvi_timeseries.shape = (x, y, t)\n",
    "    \"\"\"\n",
    "    \n",
    "    list_region = os.listdir(dir)\n",
    "    ndvi_stack = []\n",
    "    rvi_stack = []\n",
    "    vh_stack = []\n",
    "    # for region in list_region:\n",
    "    #     child_region = (os.listdir(dir + region)[0]).split('_')[0]\n",
    "        \n",
    "    #     ndvi_raster_path = dir + region + f'/{child_region}_ndvi8days'\n",
    "    #     sar_raster_path = dir + region + f'/{child_region}_rvi_8days'\n",
    "    #     ndvi_time_series_path = dir + region + '/ndvi_timeseries.npy'\n",
    "    #     rvi_time_series_path = dir + region + '/rvi_timeseries.npy'\n",
    "    #     vh_time_series_path = dir + region + '/vh_timeseries.npy'\n",
    "        \n",
    "    #     missing_date = find_missing_date(os.listdir(ndvi_raster_path), os.listdir(sar_raster_path))\n",
    "    #     print(missing_date)\n",
    "    #     create_ndvi_time_series(folder_path=ndvi_raster_path, output_path=ndvi_time_series_path, missing_dates=missing_date)\n",
    "    #     create_sar_time_series(folder_path=sar_raster_path, output_rvi_path=rvi_time_series_path, output_vh_path=vh_time_series_path)\n",
    "\n",
    "    #     fillNA(rvi_data_path=rvi_time_series_path, vh_data_path=vh_time_series_path,\n",
    "    #     rvi_full_path=rvi_time_series_path, vh_full_path=vh_time_series_path)\n",
    "    # child_region = 'tamhiep'\n",
    "    # region = 'TamHiep-ChauThanh-TienGiang'\n",
    "    \n",
    "    # ndvi_raster_path = dir + region + f'/{child_region}_ndvi8days'\n",
    "    # sar_raster_path = dir + region + f'/{child_region}_rvi_8days'\n",
    "    # ndvi_time_series_path = dir + region + '/ndvi_timeseries.npy'\n",
    "    # rvi_time_series_path = dir + region + '/rvi_timeseries.npy'\n",
    "    # vh_time_series_path = dir + region + '/vh_timeseries.npy'\n",
    "    \n",
    "    # missing_date, date_rvi = find_missing_date(os.listdir(ndvi_raster_path), os.listdir(sar_raster_path))\n",
    "    # print(missing_date)\n",
    "    # create_ndvi_time_series(folder_path=ndvi_raster_path, output_path=ndvi_time_series_path, missing_dates=missing_date, date_rvi=date_rvi)\n",
    "    # create_sar_time_series(folder_path=sar_raster_path, output_rvi_path=rvi_time_series_path, output_vh_path=vh_time_series_path)\n",
    "\n",
    "    # fillNA(rvi_data_path=rvi_time_series_path, vh_data_path=vh_time_series_path,\n",
    "    # rvi_full_path=rvi_time_series_path, vh_full_path=vh_time_series_path)\n",
    "    for region in tqdm(list_region, desc=\"load region: \"):\n",
    "        ndvi_time_series_path = dir + region + '/ndvi_timeseries.npy'\n",
    "        rvi_time_series_path = dir + region + '/rvi_timeseries.npy'\n",
    "        vh_time_series_path = dir + region + '/vh_timeseries.npy'\n",
    "        ndvi_data_ = np.load(ndvi_time_series_path)\n",
    "        rvi_data_ = np.load(rvi_time_series_path)\n",
    "        vh_data_ = np.load(vh_time_series_path)\n",
    "      \n",
    "        ndvi_data_ = ndvi_data_.reshape((ndvi_data_.shape[0] * ndvi_data_.shape[1], ndvi_data_.shape[2]))\n",
    "        rvi_data_ = rvi_data_.reshape((rvi_data_.shape[0] * rvi_data_.shape[1], rvi_data_.shape[2]))\n",
    "        vh_data_ = vh_data_.reshape((vh_data_.shape[0] * vh_data_.shape[1], vh_data_.shape[2]))\n",
    "        print(ndvi_data_.shape)\n",
    "        ndvi_stack.append(ndvi_data_)\n",
    "        rvi_stack.append(rvi_data_)\n",
    "        vh_stack.append(vh_data_)\n",
    "        \n",
    "    ndvi_data = np.concatenate(ndvi_stack, axis=0)  # Kết hợp theo chiều 0 (dọc)\n",
    "    rvi_data = np.concatenate(rvi_stack, axis=0)    # Kết hợp theo chiều 0 (dọc)\n",
    "    vh_data = np.concatenate(vh_stack, axis=0) \n",
    "    print(ndvi_data.shape)\n",
    "\n",
    "    del ndvi_stack, rvi_stack, vh_stack\n",
    "    \"\"\"\n",
    "    Step make cloudmask\n",
    "    - `ndvi_data[i]` == `np.nan` => have cloud\n",
    "    - `cloudMask[i] = 0`: no cloud \n",
    "    - `cloudMask[i]` = 1: cloud\n",
    "    \"\"\"\n",
    "    cloudMask = np.zeros(ndvi_data.shape)\n",
    "\n",
    "    for series_index in range(ndvi_data.shape[0]):\n",
    "        for time_step in range(ndvi_data.shape[1]):\n",
    "            if np.isnan(ndvi_data[series_index, time_step]):\n",
    "                cloudMask[series_index, time_step] = 1\n",
    "\n",
    "    \"\"\"\n",
    "    Make train and valid data\n",
    "    \"\"\"\n",
    "    data_num = np.full(ndvi_data.shape[0], ndvi_data.shape[1])\n",
    "    for series_index in range(cloudMask.shape[0]):\n",
    "        num_ones = len(np.argwhere(cloudMask[series_index] == 1))\n",
    "        data_num[series_index] -= num_ones\n",
    "    \n",
    "    # mask sar, 0: sar get nan, 1: fully data\n",
    "    unvalid_rvi = np.where(np.isnan(rvi_data).any(axis=1), 1, 0)\n",
    "    unvalid_vh = np.where(np.isnan(vh_data).any(axis=1), 1, 0)\n",
    "    combined_mask = np.where(np.logical_or(unvalid_rvi == 1, unvalid_vh == 1), 1, 0)\n",
    "\n",
    "    index_for_train = np.where((data_num >= 32) & (combined_mask == 0))[0]\n",
    "    cloudMask0 = cloudMask\n",
    "\n",
    "    for series_index in index_for_train:\n",
    "        uncloud_index = np.where(cloudMask0[series_index] == 0)[0]\n",
    "        fakecloud_index = choose_data_validate(uncloud_index)\n",
    "        cloudMask0[series_index, fakecloud_index] = 2\n",
    "\n",
    "\n",
    "    ndvi_input = ndvi_data[index_for_train]\n",
    "    rvi_input = rvi_data[index_for_train]\n",
    "    vh_input = vh_data[index_for_train]\n",
    "    cloudmask_input = cloudMask0[index_for_train]\n",
    "\n",
    "    print(f\"ndvi_input: {ndvi_data}\")\n",
    "    print(f\"ndvi first: {ndvi_data.shape}\")\n",
    "    print(f\"index for train: {len(index_for_train)}\")\n",
    "    print(f\"ndvi input shape: {ndvi_input.shape}\")\n",
    "    print(f\"rvi input shape: {rvi_input.shape}\")\n",
    "    print(f\"vh input shape: {vh_input.shape}\")\n",
    "\n",
    "    \"\"\"\n",
    "    have dataset contains: \n",
    "    - input data: [rvi, vh, ndvi] (n_series, n_features, n_timesteps)\n",
    "    - mask_train: 1 => training, 0 => no\n",
    "    - mask_eval: 1 => validating, 0 => no\n",
    "    \"\"\"\n",
    "    mask_train = np.where(cloudmask_input == 0, 1, 0)\n",
    "    mask_eval = np.where(cloudmask_input == 2, 1, 0)\n",
    "    # input_data = np.stack([rvi_input, vh_input, ndvi_input], axis=1)\n",
    "\n",
    "    feature_num = 3\n",
    "    n_timesteps = 46\n",
    "    time = np.arange(1,369,8) # timestep: 8\n",
    "\n",
    "    deltaT_forward = np.zeros((len(index_for_train), n_timesteps))\n",
    "    for i in range(len(index_for_train)):\n",
    "        maskone = mask_train[i, :]\n",
    "        done = cal_timestep(time, maskone)\n",
    "        deltaT_forward[i, :] = done\n",
    "\n",
    "    deltaT_backward = np.zeros((len(index_for_train), n_timesteps))\n",
    "    for i in range(len(index_for_train)):\n",
    "        maskone = mask_train[i, :]\n",
    "        maskone = maskone[::-1]\n",
    "        done = cal_timestep(time, maskone)\n",
    "        deltaT_backward[i, :] = done\n",
    "\n",
    "    print('Generate time interval for SAR data: ')\n",
    "    deltaTt = np.zeros((len(index_for_train), n_timesteps))\n",
    "    for i in range(len(index_for_train)):\n",
    "        maskone = np.ones(n_timesteps)\n",
    "        maskone = np.int_(maskone)\n",
    "        done = cal_timestep(time, maskone)\n",
    "        deltaTt[i, :] = done\n",
    "\n",
    "    # ndvi_input_backward = ndvi_input[:, ::-1]\n",
    "    # rvi_input_backward = rvi_input[:, ::-1]\n",
    "    # vh_input_backward = vh_input[:, ::-1]\n",
    "\n",
    "    # input_data_backward = np.stack([rvi_input_backward, vh_input_backward, ndvi_input_backward], axis=1)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Final create dataset\n",
    "    \"\"\"\n",
    "    traindatasets_valuesF = np.empty((n_timesteps, feature_num, 0),dtype=np.float16)\n",
    "    traindatasets_evalmaskF = np.empty((n_timesteps, 0),dtype=np.int8)\n",
    "    traindatasets_maskF = np.empty((n_timesteps, 0),dtype=np.int8)\n",
    "    traindatasets_deltaF = np.empty((n_timesteps, feature_num, 0),dtype=np.float16)\n",
    "    traindatasets_deltaBF = np.empty((n_timesteps, feature_num, 0),dtype=np.float16)\n",
    "\n",
    "\n",
    "    print('Generate training datasets: ')\n",
    "    trainingdatasets_values = np.zeros((n_timesteps, feature_num, len(index_for_train)))\n",
    "    for step in range(n_timesteps):\n",
    "        for series_index in range(len(index_for_train)):\n",
    "            trainingdatasets_values[step, 0, series_index] = vh_input[series_index, step]\n",
    "            trainingdatasets_values[step, 1, series_index] = rvi_input[series_index, step]\n",
    "            if np.isnan(ndvi_input[series_index, step]):\n",
    "                trainingdatasets_values[step, 2, series_index] = -100\n",
    "            else:\n",
    "                trainingdatasets_values[step, 2, series_index] = ndvi_input[series_index, step]\n",
    "    traindatasets_valuesF = np.concatenate((traindatasets_valuesF, trainingdatasets_values), axis=2)\n",
    "\n",
    "    print('Generate evalmask: ')\n",
    "    traindatasets_evalmask = np.zeros((n_timesteps,len(index_for_train)), dtype=np.int8)\n",
    "    for step in range(n_timesteps):\n",
    "        for series_index in range(len(index_for_train)):\n",
    "            traindatasets_evalmask[step, series_index] = mask_eval[series_index, step]\n",
    "    traindatasets_evalmaskF = np.concatenate((traindatasets_evalmaskF, traindatasets_evalmask), axis=1)\n",
    "\n",
    "    print('Generate mask tran: ')\n",
    "    traindatasets_mask = np.zeros((n_timesteps, len(index_for_train)), dtype=np.int8)\n",
    "    for step in range(n_timesteps):\n",
    "        for series_index in range(len(index_for_train)):\n",
    "            traindatasets_mask[step, series_index] = mask_train[series_index, step]\n",
    "    traindatasets_maskF = np.concatenate((traindatasets_maskF, traindatasets_mask), axis=1)\n",
    "\n",
    "    print('Generate training delta datasets: ')\n",
    "    traindatasets_delta = np.zeros((n_timesteps, feature_num, len(index_for_train)), dtype=np.float16)\n",
    "    for step in range(n_timesteps):\n",
    "        for series_index in range(len(index_for_train)):\n",
    "            traindatasets_delta[step, 0, series_index] = deltaTt[series_index, step]\n",
    "            traindatasets_delta[step, 1, series_index] = deltaTt[series_index, step]\n",
    "            traindatasets_delta[step, 2, series_index] = deltaT_forward[series_index, step]\n",
    "    traindatasets_deltaF = np.concatenate((traindatasets_deltaF, traindatasets_delta), axis=2)\n",
    "\n",
    "    print('Generate training delta dataset backward: ')\n",
    "    traindatasets_delta_backward = np.zeros((n_timesteps, feature_num, len(index_for_train)), dtype=np.float16)\n",
    "    for step in range(n_timesteps):\n",
    "        for series_index in range(len(index_for_train)):\n",
    "            traindatasets_delta_backward[step, 0, series_index] = deltaTt[series_index, step]\n",
    "            traindatasets_delta_backward[step, 1, series_index] = deltaTt[series_index, step]\n",
    "            traindatasets_delta_backward[step, 2, series_index] = deltaT_backward[series_index, step]\n",
    "    traindatasets_deltaBF = np.concatenate((traindatasets_deltaBF, traindatasets_delta_backward), axis=2)\n",
    "\n",
    "\n",
    "    fs = open(dir + region + '/training_data.json', 'w')\n",
    "    all_len = traindatasets_valuesF.shape[2]\n",
    "    print('Save training dataset as JSON: ')\n",
    "    for id_ in tqdm(range(all_len)):\n",
    "        parse_idTrain(id_)\n",
    "    fs.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:   6%|▋         | 1/16 [00:01<00:28,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288156, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  12%|█▎        | 2/16 [00:10<01:18,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1289523, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  19%|█▉        | 3/16 [00:11<00:50,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254745, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  25%|██▌       | 4/16 [00:15<00:43,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(639944, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  31%|███▏      | 5/16 [00:17<00:35,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250985, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  38%|███▊      | 6/16 [00:22<00:38,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(511947, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  44%|████▍     | 7/16 [01:23<03:20, 22.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3349290, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  50%|█████     | 8/16 [01:26<02:10, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193500, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  56%|█████▋    | 9/16 [01:30<01:26, 12.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181832, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  62%|██████▎   | 10/16 [01:35<01:00, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(298112, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  69%|██████▉   | 11/16 [01:38<00:39,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106790, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  75%|███████▌  | 12/16 [01:44<00:29,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(334620, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  81%|████████▏ | 13/16 [01:53<00:23,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(490912, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  88%|████████▊ | 14/16 [01:55<00:12,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93906, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region:  94%|█████████▍| 15/16 [02:04<00:07,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(541280, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load region: 100%|██████████| 16/16 [02:07<00:00,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165000, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8990542, 46)\n",
      "ndvi_input: [[       nan        nan 0.68167245 ... 0.74069685 0.71910316 0.71910316]\n",
      " [       nan        nan 0.6102041  ... 0.75205183 0.70687234 0.70687234]\n",
      " [       nan        nan 0.60912228 ... 0.70671016 0.68156332 0.68156332]\n",
      " ...\n",
      " [0.74084091 0.74084091        nan ...        nan 0.79320753 0.78629035]\n",
      " [0.68284792 0.68284792        nan ...        nan 0.80085278 0.80085278]\n",
      " [0.68757194 0.68757194        nan ...        nan 0.81469035 0.81469035]]\n",
      "ndvi first: (8990542, 46)\n",
      "index for train: 1739126\n",
      "ndvi input shape: (1739126, 46)\n",
      "rvi input shape: (1739126, 46)\n",
      "vh input shape: (1739126, 46)\n",
      "Generate time interval for SAR data: \n",
      "Generate training datasets: \n",
      "Generate evalmask: \n",
      "Generate mask tran: \n",
      "Generate training delta datasets: \n",
      "Generate training delta dataset backward: \n",
      "Save training dataset as JSON: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1739126/1739126 [24:32<00:00, 1180.70it/s]\n"
     ]
    }
   ],
   "source": [
    "create_json_data(dir='/mnt/storage/huyekgis/brios/datasets/TRAINING_Data4BRIOS_4M/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('/mnt/storage/huyekgis/brios/datasets/TRAINING_Data4BRIOS_4M/TamHiep-ChauThanh-TienGiang/ndvi_timeseries.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585, 572, 46)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
